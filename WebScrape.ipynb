{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pprint as pp\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "from numpy.random import choice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths for data caching\n",
    "dataDir = 'data'\n",
    "if not os.path.exists(dataDir):\n",
    "    os.makedirs(dataDir)\n",
    "urlListFName = '%s/urls.txt' % dataDir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wineoneline.com.au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise web fetching stuff\n",
    "domain = \"http://www.wineonline.com.au\"\n",
    "dataDirThisDomain = dataDir + '/wineonline'\n",
    "woAllTextFname = '%s/all.txt' % dataDirThisDomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape list of wines to get a list of urls to individual wines\n",
    "if not os.path.exists(urlListFName):\n",
    "    individualPages = []\n",
    "\n",
    "    # get list of wine pages\n",
    "    for wine_type in ['sparkling','white-wine','red-wine','imported']:\n",
    "        print(\"Start of wine_type %s\" % wine_type)\n",
    "        page = 0\n",
    "        while True:\n",
    "            suffix = '?sort=alphaasc&page=%d' % page\n",
    "            url = '/'.join([domain,wine_type,suffix])\n",
    "            r = http.request('GET',url)\n",
    "            if r.status == 200:\n",
    "                print(\"Hit: \" + url)\n",
    "                html = r.data.decode('utf-8')\n",
    "                soup = bs(html, 'html.parser')\n",
    "                individualPages += [a.get('href') for a in soup.select('.ProdHeading a')]\n",
    "                page = page + 1\n",
    "\n",
    "            else:\n",
    "                print(\"end of %s at %d\" % (wine_type, page))\n",
    "                break\n",
    "    individualPages = list(set(individualPages))\n",
    "    print(\"Writing list of urls to %s\" % urlListFName)\n",
    "    with open(urlListFName,'w') as f:\n",
    "        f.write('\\n'.join(individualPages))\n",
    "else:\n",
    "    print(\"Skipping fetching from the search results pages\")\n",
    "print(\"Reading in from %s\" % urlListFName)\n",
    "with open(urlListFName,'r') as f:\n",
    "    individualPages = [line.strip() for line in f.read().split('\\n') if line.strip() != '']\n",
    "#individualPages = individualPages[:10] # only some for now\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [url.rstrip('/').split('/')[-1] for url in individualPages]\n",
    "assert(len(paths) == len(individualPages))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# scrape all the wines in the list\n",
    "\n",
    "def fetchPath(path):\n",
    "#     print(\"fetching %s\" % url)\n",
    "    url = domain.rstrip('/') + '/' + path.lstrip('/')\n",
    "    dirName = '%s/bottles/%s' % (dataDirThisDomain,path)\n",
    "    if not os.path.exists(dirName):\n",
    "        os.makedirs(dirName)\n",
    "    htmlFName = '%s/%s' % (dirName,'html.html')\n",
    "    if not os.path.exists(htmlFName):\n",
    "        print(\"Fetching %s\" % url)\n",
    "        r = http.request('GET',url)\n",
    "        if r.status == 200:\n",
    "    #         print(\"Parsing %s\" % url)\n",
    "            html = r.data.decode('utf-8').replace(u'\\xa0', u' ')\n",
    "            print(\"Saving to %s\" % htmlFName)\n",
    "            with open(htmlFName,'w') as f:\n",
    "                f.write(html)\n",
    "        else:\n",
    "            print(\"Error, status %d for url %s\" % (r.status,url))\n",
    "            return([])\n",
    "    else:\n",
    "        print(\"Hit cache for %s\" % htmlFName)\n",
    "    with open(htmlFName,'r') as f:\n",
    "        html = f.read()\n",
    "    return(html)\n",
    "    \n",
    "\n",
    "if not os.path.exists(woAllTextFname):\n",
    "    with Pool(20) as p:\n",
    "        htmls = p.map(fetchPath,paths)\n",
    "    print(\"done\")\n",
    "else:\n",
    "    print(\"Skipping, because I'll read from cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(woAllTextFname):\n",
    "    paras = []\n",
    "    soups = [bs(html) for html in htmls]\n",
    "    for soup in soups:\n",
    "        print('next soup')\n",
    "        paras += soup.select('#ProductDescription p')\n",
    "\n",
    "    print(\"done\")\n",
    "else:\n",
    "    print(\"Skipping, because I'll read from cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(woAllTextFname):\n",
    "    for para in paras:\n",
    "        for tag in ['span','strong','p']:\n",
    "            for el in para.find_all(tag):\n",
    "                if el.string:\n",
    "                    num_words = len(el.string.replace('\\n',' ').split(' '))\n",
    "                    if num_words < 15:\n",
    "                        el.string.replace_with('')\n",
    "else:\n",
    "    print(\"Skipping, because I'll read from cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(woAllTextFname):\n",
    "    text = '\\n'.join([para.get_text() for para in paras]).replace(u'\\xa0',' ')\n",
    "    for c in '.?!':\n",
    "        text = text.replace(c,c + ' ').replace(c + '  ',c + ' ')\n",
    "    with open(woAllTextFname,'w') as f:\n",
    "        f.write(text)\n",
    "\n",
    "with open(woAllTextFname,'r') as f:\n",
    "    allTextWO = f.read()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nakedwines.com.au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found the number 210 through trial and error\n",
    "# bigger number means more results per page\n",
    "# but their crappy site fails when you enter a number too big (e.g. 210)\n",
    "domain = 'https://www.nakedwines.com.au'\n",
    "\n",
    "dataDirThisDomain = dataDir + '/nakedwines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSearchResults(searchPage):\n",
    "    url = domain + \"/wines/index?searchText=&sortWines=descprice&pageSize=20&view=Wines&layoutType=card&allWines=true&pageNum=%d\" % searchPage\n",
    "    print(\"Fetching \" + url)\n",
    "    r = http.request('GET',url)\n",
    "    assert(r.status == 200)\n",
    "    print(\"Hit: \" + url)\n",
    "    html = r.data.decode('utf-8')\n",
    "    soup = bs(html, 'html.parser')\n",
    "    links = [a.get('href') for a in soup.select('a.card__header')]\n",
    "    print(\"Found %d results for searchPage %d\" % (len(links),searchPage))\n",
    "    return(links)\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(dataDirThisDomain):\n",
    "    os.makedirs(dataDirThisDomain)\n",
    "fname = '%s/paths.txt' % dataDirThisDomain\n",
    "\n",
    "if not os.path.exists(fname):\n",
    "    print(\"No cache found, fetching from search results on \" + domain)\n",
    "    searchPage = 0\n",
    "    results = getSearchResults(searchPage)\n",
    "    individualPages = results\n",
    "    while len(results) > 0:\n",
    "        searchPage += 1\n",
    "        results = getSearchResults(searchPage)\n",
    "        individualPages += results\n",
    "    with open(fname,'w') as f:\n",
    "        f.write('\\n'.join(individualPages))\n",
    "    # write and then read back from file\n",
    "    # to make sure we wrote correctly\n",
    "\n",
    "print(\"Reading urls from cache\")\n",
    "with open(fname,'r') as f:\n",
    "    individualPages = [x.strip() for x in f.read().split('\\n') if x.strip() != '']\n",
    "    \n",
    "\n",
    "print(\"Using urls:\")\n",
    "print('\\n'.join(individualPages[:3] + ['...'])) \n",
    "print(\"Discovered %d individual pages\" % len(individualPages))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetchPath(path):\n",
    "    url = domain + path\n",
    "    print(\"Fetching \" + url)\n",
    "    r = http.request('GET',url)\n",
    "    assert(r.status == 200)\n",
    "    print(\"Hit: \" + url)\n",
    "    html = r.data.decode('utf-8')\n",
    "    fname = dataDirThisDomain.rstrip('/') + '/' + path.lstrip('/') + '.html'\n",
    "    directory = '/'.join(fname.split('/')[:-1])\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    with open(fname,'w') as f:\n",
    "        f.write(html)\n",
    "    soup = bs(html, 'html.parser')\n",
    "    textEls = soup.find(id=\"tab-tick\")\n",
    "    assert(textEls)\n",
    "    text = textEls.get_text().replace(u'\\xa0',' ').strip()\n",
    "    for c in '.?!':\n",
    "        text = text.replace(c,c + ' ').replace(c + '  ',c + ' ')\n",
    "    return(text)\n",
    "\n",
    "fname = '%s/all.txt' % dataDirThisDomain \n",
    "if not os.path.exists(fname):\n",
    "    print(\"No cache, fetching\")\n",
    "    with Pool(20) as p:\n",
    "        texts = p.map(fetchPath,individualPages)\n",
    "    with open(fname,'w') as f:\n",
    "        f.write('\\n'.join(texts))\n",
    "else:\n",
    "    print(\"hit cache\")\n",
    "    \n",
    "with open(fname,'r') as f:\n",
    "    allTextNW = f.read()\n",
    "\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combining web scrapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allText = allTextWO + '\\n' + allTextNW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a string, returns an array of words\n",
    "# where punctuation is a 'word'\n",
    "def toWords(s):\n",
    "    s = s.replace('\\n',' ')\n",
    "    s = s.replace('\\t',' ')\n",
    "    words = []\n",
    "    for chunk in s.split(' '):\n",
    "        chunk = chunk.lstrip('([{\\'').rstrip(')]}').strip('`â€˜\"\"')\n",
    "        if chunk.strip() != '':\n",
    "            punctuation = ',.?!;'\n",
    "            for p in punctuation:\n",
    "                if chunk.endswith(p):\n",
    "                    words.append(chunk[:-1])\n",
    "                    words.append(p)\n",
    "            if not(any([chunk.endswith(p) for p in punctuation])):\n",
    "                words.append(chunk)\n",
    "              \n",
    "    wordsS = set(words)\n",
    "    func = lambda x: x.lower() if x.lower() in wordsS else x\n",
    "    words = [func(w) for w in words]\n",
    "    return(words)\n",
    "\n",
    "def testToWords():\n",
    "    s = 'At the dawn'\n",
    "    words = ['At','the','dawn']\n",
    "    assert(words==toWords(s))\n",
    "    \n",
    "    s = 'Of a  new  age\\n'\n",
    "    words = ['Of','a','new','age']\n",
    "    assert(words==toWords(s))\n",
    "    \n",
    "    s = 'At the dawn, of  a\\nnew \\n age!'\n",
    "    expected = ['At','the','dawn',',','of','a','new','age','!']\n",
    "    actual = toWords(s)\n",
    "    if expected != actual:\n",
    "        print(\"expected: \")\n",
    "        pp.pprint(expected)\n",
    "        print(\"actual: \")\n",
    "        pp.pprint(actual)\n",
    "    assert(expected==actual)\n",
    "    \n",
    "    s = 'Here is here'\n",
    "    words = ['here','is','here']\n",
    "    assert(words==toWords(s))\n",
    "    \n",
    "    s = 'Here is (bracket stuff)'\n",
    "    words = ['Here','is','bracket','stuff']\n",
    "    if words != toWords(s):\n",
    "        print(toWords(s))\n",
    "    assert(words==toWords(s))\n",
    "    \n",
    "    s = 'Here is \"a quote\"'\n",
    "    words = ['Here','is','a','quote']\n",
    "    assert(words==toWords(s))\n",
    "    \n",
    "    s = \"Ha! That's funny\"\n",
    "    words = ['Ha','!',\"That's\",'funny']\n",
    "    assert(words==toWords(s))\n",
    "    \n",
    "testToWords()\n",
    "print(\"test passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTextSplit = toWords(allText)\n",
    "print(allTextSplit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataOne = {}\n",
    "for i in range(1,len(allTextSplit)):\n",
    "    if (i % 10000) == 0:\n",
    "        print('...')\n",
    "    thisWord = allTextSplit[i]\n",
    "    prevWord = allTextSplit[i-1]\n",
    "    if prevWord == '.':\n",
    "        prevWord = '' # use empty string to represent start\n",
    "    if prevWord not in dataOne:\n",
    "        dataOne[prevWord] = {thisWord:1}\n",
    "    elif thisWord in dataOne[prevWord]:\n",
    "        dataOne[prevWord][thisWord] += 1\n",
    "    else:\n",
    "        dataOne[prevWord][thisWord] = 1\n",
    "        \n",
    "# remove single character sentences\n",
    "for c in '?!.;':\n",
    "    if c in dataOne['']:\n",
    "        del dataOne[''][c]\n",
    "print(\"done\")\n",
    "dataOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '%s/dataOne.json' % dataDir\n",
    "with open(fname,'w') as f:\n",
    "    json.dump(dataOne,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is a dictionary\n",
    "# the keys are what we are choosing\n",
    "# the values are the weights\n",
    "def weightedRandom(dist):\n",
    "    entries = list(dist.keys())\n",
    "    assert(len(entries) > 0)\n",
    "    probabilities = [dist[e] for e in entries]\n",
    "    scale = sum(probabilities)\n",
    "    probabilities = [float(p)/scale for p in probabilities]\n",
    "    draw = choice(range(len(entries)), 1, p=probabilities)[0]\n",
    "    return(entries[draw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentenceOne():\n",
    "    word = weightedRandom(dataOne['']) # first word\n",
    "    words = [word]\n",
    "    while word not in '?!.;':\n",
    "        prevWord = words[-1]\n",
    "        word = weightedRandom(dataOne[prevWord])\n",
    "        words.append(word)\n",
    "    \n",
    "    sentence = ''\n",
    "    for word in words:\n",
    "        if sentence == '':\n",
    "            if len(word) == 1:\n",
    "                sentence = word.upper()\n",
    "            else:\n",
    "                sentence = word[0].upper() + word[1:]\n",
    "        else:\n",
    "            if (word not in '-.,?;!'):\n",
    "                sentence += ' '\n",
    "            sentence += word\n",
    "    if len(sentence) < 10:\n",
    "        sentence = generateSentenceOne()\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[generateSentenceOne() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N Layer chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numLayers = 2\n",
    "assert(numLayers > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split allTextSplit into a list of sentences, where each sentence is a list of words\n",
    "sentences = [[]]\n",
    "for word in allTextSplit:\n",
    "    sentences[-1].append(word)\n",
    "    if word in '?!.;-':\n",
    "        sentences.append([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNStart = {}\n",
    "dataN = {}\n",
    "solo = 0\n",
    "many = 0\n",
    "for sentence in sentences:\n",
    "    if len(sentence) > numLayers:\n",
    "        # get the start of the sentence\n",
    "        firstN = tuple(sentence[0:numLayers])\n",
    "        if firstN in dataNStart:\n",
    "            dataNStart[firstN] += 1\n",
    "        else:\n",
    "            dataNStart[firstN] = 1\n",
    "\n",
    "        # now the rest\n",
    "        for i in range(numLayers,len(sentence)):\n",
    "            thisWord = sentence[i]\n",
    "            prevWords = tuple(sentence[i-numLayers:i])\n",
    "            if prevWords not in dataN:\n",
    "                dataN[prevWords] = {thisWord:1}\n",
    "            elif thisWord not in dataN[prevWords]:\n",
    "                dataN[prevWords][thisWord] = 1\n",
    "                solo += 1\n",
    "            else:\n",
    "                dataN[prevWords][thisWord] += 1\n",
    "                many += 1\n",
    "\n",
    "\n",
    "assert(all([type(k) == type((1,2)) for k in dataN.keys()]))\n",
    "print(\"solo: %d\" % solo)\n",
    "print(\"many: %d\" % many)\n",
    "assert(all([k in dataN for k in dataNStart]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting\")\n",
    "# This is pretty messy\n",
    "\n",
    "# I want to remove all words for which filter(word) returns True\n",
    "# but then I have to remove all paths which lead to only that word\n",
    "# but then I have to remove all paths which lead to only those paths\n",
    "# etc.\n",
    "\n",
    "def filter(word):\n",
    "    names = ['Ben','James','Tyson','Steve','Andrew','Campbell','Jen','Margaret','Nigel', 'Kim','James-Paul','Gary']\n",
    "    for name in names:\n",
    "        if word in [name,name + \"'s\"]:\n",
    "            return(True)\n",
    "    if word.lower() in ['angel',\"angel's\",\"angels\",'naked']:\n",
    "        # Naked Wines mentions these a lot\n",
    "        return(True)\n",
    "    elif word.lower() in ['points']:\n",
    "        # wineonline mentions these\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "# returns a dict\n",
    "# {'start':[],'main':{'keys':[],'leafs':{k:[]}},'count':x}\n",
    "# Where that first list is the list of keys to remove from the start dict\n",
    "# And the 2nd is the list of keys to remove from the outer level of the main dict\n",
    "# And the 3rd is the list of keys to remove from the inner level of the main dict\n",
    "def getToPrune(startData,mainData):\n",
    "    ret = {}\n",
    "    ret['start'] = set([words for words in startData if any([filter(word) for word in words]) or words not in mainData])\n",
    "    ret['count'] = len(ret['start'])\n",
    "    print(\"Populated ret[start] with %d\" % ret['count'])\n",
    "    ret['main'] = {'leafs':{},'keys':set()}\n",
    "    for prevWords in mainData: # a tuple of words\n",
    "        #print(\"running getToPrune with prevWords = \" + str(prevWords))\n",
    "        # TODO: check if these leafs exist as keys\n",
    "        for nextWord in mainData[prevWords]: \n",
    "            \n",
    "            # if this word is in the filter, delete it\n",
    "            # if choosing this word will give a chain that leads to a dead end, delete it\n",
    "            newChain = (prevWords + (nextWord,))[1:]\n",
    "            if filter(nextWord) or ((newChain not in mainData) and (nextWord not in '.!?')):\n",
    "                if prevWords not in ret['main']['leafs']:\n",
    "                    ret['main']['leafs'][prevWords] = set([nextWord])\n",
    "                else:\n",
    "                    ret['main']['leafs'][prevWords].add(nextWord)\n",
    "#                 if filter(nextWord):\n",
    "#                     print(\"Incrementing count for mainData[%s][%s] because %s is filtered\" % (str(prevWords),nextWord,nextWord))\n",
    "#                 else:\n",
    "#                     print(\"Incrementing count for mainData[%s][%s] because %s a dead end\" % (str(prevWords),nextWord,newChain))\n",
    "                ret['count'] += 1\n",
    "        \n",
    "        # if we are about to delete all entries for mainData[prevWords], delete prevWords from mainData\n",
    "        deletePrevWords = (prevWords in ret['main']['leafs']) and (len(ret['main']['leafs'][prevWords]) == len(mainData[prevWords]))\n",
    "        \n",
    "        # if mainData[prevWords] is already empty, delete it\n",
    "        deletePrevWords |= len(mainData[prevWords]) == 0\n",
    "        \n",
    "        if deletePrevWords:\n",
    "            ret['main']['keys'].add(prevWords)\n",
    "#             print(\"Incrementing count for words %s\" % str(prevWords))\n",
    "            ret['count'] += 1\n",
    "        elif (len(mainData[prevWords]) == 0):\n",
    "#             print(\"Incrementing count for words %s\" % str(prevWords))\n",
    "            ret['main']['keys'].add(prevWords)\n",
    "            ret['count'] += 1\n",
    "    return(ret)\n",
    "    \n",
    "print(\"Starting pruning\")\n",
    "toPrune = getToPrune(dataNStart,dataN)\n",
    "# toPrune\n",
    "while toPrune['count'] > 0:\n",
    "#     print('pruning (count = %d)' % toPrune['count'])\n",
    "#     print(\"pruning %d from start dict\" % len(toPrune['start']))\n",
    "    assert(all([k in dataNStart for k in toPrune['start']]))\n",
    "    dataNStart = {k:dataNStart[k] for k in dataNStart if k not in toPrune['start']}\n",
    "    \n",
    "#     print(\"pruning %d from main leafs\" % sum([len(toPrune['main']['leafs'][k]) for k in toPrune['main']['leafs']]))\n",
    "    for k in toPrune['main']['leafs']:\n",
    "        assert(k in dataN)\n",
    "        assert(all([j in dataN[k] for j in toPrune['main']['leafs'][k]]))\n",
    "        dataN[k] = {j:dataN[k][j] for j in dataN[k] if j not in toPrune['main']['leafs'][k]}\n",
    "        \n",
    "#     print(\"pruning %d from main keys\" % len(toPrune['main']['keys']))\n",
    "    assert(all([k in dataN for k in toPrune['main']['keys']]))\n",
    "    dataN = {k:dataN[k] for k in dataN if k not in toPrune['main']['keys']}\n",
    "    toPrune = getToPrune(dataNStart,dataN)\n",
    "    \n",
    "solo = 0\n",
    "many = 0\n",
    "for k in dataN:\n",
    "    if len(dataN[k]) == 1:\n",
    "        solo += 1\n",
    "    else:\n",
    "        many += 1\n",
    "        \n",
    "print(\"Solo: %d\" % solo)\n",
    "print(\"Many: %d\" % many)\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalise(word):\n",
    "    if len(word) == 1:\n",
    "        return(word.upper())\n",
    "    else:\n",
    "        return(word[0].upper() + word[1:])\n",
    "\n",
    "assert(capitalise('a') == 'A')\n",
    "assert(capitalise('hello') == 'Hello')\n",
    "assert(capitalise('A') == 'A')\n",
    "assert(capitalise('Hello') == 'Hello')\n",
    "    \n",
    "def generateSentenceMany():\n",
    "    words = list(weightedRandom(dataNStart))\n",
    "    while words[-1] not in '!?.':\n",
    "        weights = dataN[tuple(words[-numLayers:])]\n",
    "        nextWord = weightedRandom(weights)\n",
    "        assert(type(nextWord) == type(''))\n",
    "        words.append(nextWord)\n",
    "        \n",
    "    sentence = capitalise(words[0])\n",
    "    for word in words[1:]:\n",
    "        if word not in ',.!?;':\n",
    "            sentence += ' '\n",
    "        sentence += word\n",
    "        \n",
    "    if not (5 < len(words) < 25):\n",
    "        # bad length, try again\n",
    "        return(generateSentenceMany())\n",
    "    else:\n",
    "        return(sentence)\n",
    "\n",
    "print('\\n\\n'.join([generateSentenceMany() for _ in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
